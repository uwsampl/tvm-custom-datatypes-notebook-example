{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4aC_aNcUP75"
   },
   "source": [
    "# Bring Your Own Datatypes to TVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "seyxX9HZUP76"
   },
   "source": [
    "In this tutorial, we will show you how you can use your own custom datatypes in TVM, utilizing TVM's Bring Your Own Datatypes framework.\n",
    "Note that the Bring Your Own Datatypes framework currently only handles **software emulated versions of datatypes** right now, which is what we'll be discussing in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datatype Libraries\n",
    "\n",
    "The central idea of the Bring Your Own Datatypes framework is to make datatypes in TVM more abstract, so that users can bring their own software-emulated datatype implementations.\n",
    "\n",
    "In the wild, these datatype implementations usually appear as libraries. For example:\n",
    "- [libposit](https://github.com/cjdelisle/libposit), a posit library\n",
    "- [Stillwater Universal](https://github.com/stillwater-sc/universal), a library with posits, fixed-point numbers, and other types\n",
    "- [SoftFloat](https://github.com/ucb-bar/berkeley-softfloat-3), Berkeley's software implementation of IEEE 754\n",
    "\n",
    "The Bring Your Own Datatypes framework allows libraries such as these to be easily plugged in to TVM!\n",
    "\n",
    "In this section, we will explore our example library, [libposit](https://github.com/cjdelisle/libposit).\n",
    "**Posits** are a datatype developed to compete with IEEE 754 floating point numbers.\n",
    "We won't go into much detail about the datatype itself.\n",
    "If you'd like to learn more, read through John Gustafson's [Beating Floating Point at its Own Game](https://posithub.org/docs/BeatingFloatingPoint.pdf).\n",
    "\n",
    "First, let's clone and build the library. Note that we're using my branch of libposit which includes some fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/uwsampl/libposit\n",
    "! cd libposit && git checkout 7c1788f291c1b5f74ded9acf4ffae7911c2df28c && autoreconf -f -i && ./configure && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library contains operations over posits.\n",
    "Let's see what we can do with 16-bit posits (which are much like 16-bit floats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nm libposit/libposit.a | grep posit16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see many operations that we might expect for datatypes: `posit16_abs` for absolute value, `posit16_add` for addition, `posit16_cmp` for comparsion, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libposit_wrapper_source = \"\"\"\n",
    "#include \"posit.h\"\n",
    "\"\"\"\n",
    "! echo {libposit_wrapper_source} > libposit-wrapper.cc\n",
    "! cc -Ilibposit/generated -lmpfr -lgmp --std=c++14 -shared -o libposit-wrapper.so -fPIC libposit-wrapper.cc libposit/libposit.a\n",
    "! ls -alh libposit-wrapper.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fxZ29ZrUP78"
   },
   "source": [
    "## A Simple TVM Program\n",
    "We'll begin by writing a simple program in TVM; afterwards, we will re-write it to use custom datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghinqYT5UP79"
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "\n",
    "# Our basic program: Z = X + Y\n",
    "X = tvm.placeholder((3, ))\n",
    "Y = tvm.placeholder((3, ))\n",
    "Z = X + Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2TjXsfqUP8A"
   },
   "source": [
    "Next, we compile for LLVM. The process of compiling in TVM is broken into scheduling, lowering, and finally, building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guuiH9frUP8B"
   },
   "outputs": [],
   "source": [
    "target = \"llvm\"\n",
    "schedule = tvm.create_schedule([Z.op])\n",
    "lowered_func = tvm.lower(schedule, [X, Y, Z])\n",
    "built_program = tvm.build(lowered_func, target=target)\n",
    "\n",
    "# Print the lowered IR (simple mode makes it cleaner to read!)\n",
    "print(tvm.lower(schedule, [X, Y, Z], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOyHNW9jUP8E"
   },
   "source": [
    "Now, we create random inputs to feed into this program using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIJNyNqJUP8F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a device context\n",
    "context = tvm.context(target, 0)\n",
    "\n",
    "# Create random input arrays on the above context\n",
    "x = tvm.nd.array(np.array([1.0, 1.0, 1.0]).astype(\"float32\"), ctx=context)\n",
    "y = tvm.nd.array(np.array([1.333, -0.75, 10.9]).astype(\"float32\"), ctx=context)\n",
    "print(\"x: {}\".format(x))\n",
    "print(\"y: {}\".format(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqgXo4ZEUP8J"
   },
   "source": [
    "This empty array will hold our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vywGxVAoUP8K"
   },
   "outputs": [],
   "source": [
    "z = tvm.nd.empty(Z.shape, dtype=Z.dtype, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fm8ueDQlUP8O"
   },
   "source": [
    "Finally, we're ready to run the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ld39h2rvUP8P"
   },
   "outputs": [],
   "source": [
    "built_program(x, y, z)\n",
    "print(\"z: {}\".format(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsEprZVMUP8S"
   },
   "source": [
    "## Interlude: `bfloat16`\n",
    "\n",
    "Before we rewrite our program using custom datatypes, let's introduce the custom datatype we will use: the `bfloat16`. `bfloat16` is a very straightforward datatype; it is simply a 32-bit IEEE float chopped in half! Specifically, the 16 least-significant bits of the fraction are chopped off. The result is a format which\n",
    "- is straightforward to convert to and from 32-bit IEEE float\n",
    "- has the same dynamic range as a 32-bit IEEE float, but with less precision\n",
    "- takes up half the space!\n",
    "\n",
    "The `bfloat16` is built in to TensorFlow, and used natively on deep learning hardware (such as the TPU). Training deep learning models with the `bfloat16` often results in the same converged accuracy, [according to TensorFlow docs!](https://cloud.google.com/tpu/docs/bfloat16)\n",
    "\n",
    "TVM has a toy `bfloat16` library built-in for testing and demonstration purposes at [3rdparty/bfloat16/bfloat16.cc](https://github.com/dmlc/tvm/blob/master/3rdparty/bfloat16/bfloat16.cc). The `float->bfloat16` and `bfloat16->float` functions are taken from TensorFlow, while the other functions simply convert to `float` and use the native implementations of the functions they implement. Thus, it is not a true `bfloat16` implementation, but serves perfectly well for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XOfXquqUP8T"
   },
   "source": [
    "## Adding Custom Datatypes\n",
    "\n",
    "Now, we will do the same, but we will use a custom datatype for our intermediate computation.\n",
    "\n",
    "We use the same input placeholders `X` and `Y` as above, but before adding `X + Y`, we first cast both `X` and `Y` to a custom datatype via the `topi.cast(...)` call.\n",
    "\n",
    "Note how we specify the custom datatype: we indicate it using the special `custom[...]` syntax. Additionally, note the \"16\" after the datatype: this is the bitwidth of the custom datatype. This tells TVM that each instance of `bfloat` is 16 bits wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHhq0Tk9UP8U"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    Z = topi.cast(\n",
    "        topi.cast(X, dtype=\"custom[bfloat]16\") +\n",
    "        topi.cast(Y, dtype=\"custom[bfloat]16\"),\n",
    "        dtype=\"float32\")\n",
    "except tvm.TVMError as e:\n",
    "    # Print last line of error\n",
    "    print(str(e).split('\\n')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJqUIBorUP8X"
   },
   "source": [
    "Trying to generate this program throws an error from TVM:\n",
    "`TVMError: Check failed: name_to_code_.find(type_name) != name_to_code_.end(): Type name bfloat not registered`.\n",
    "Unsurprisingly, TVM does not know how to handle any custom datatype out of the box. We first have to register the custom type with TVM, giving it a name and a type code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJKVCD7OUP8Y"
   },
   "outputs": [],
   "source": [
    "tvm.datatype.register(\"bfloat\", 129)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HJruYl15UP8b"
   },
   "source": [
    "Note that the type code, 129, is currently chosen manually by the programmer. See `TVMTypeCode::kCustomBegin` in [include/tvm/runtime/c_runtime_api.h](https://github.com/dmlc/tvm/blob/master/include/tvm/runtime/c_runtime_api.h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QI96dWiIUP8c"
   },
   "source": [
    "Now we can generate our program again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wZWHxd7UP8f"
   },
   "outputs": [],
   "source": [
    "Z = topi.cast(\n",
    "    topi.cast(X, dtype=\"custom[bfloat]16\") +\n",
    "    topi.cast(Y, dtype=\"custom[bfloat]16\"),\n",
    "    dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSq-jAPRUP8h"
   },
   "source": [
    "Next, we again compile our program by scheduling, lowering, and building.\n",
    "\n",
    "Note that we currently have to manually lower custom datatypes via the `tvm.ir_pass.LowerCustomDatatypes(...)` call. This is simply because we have not incorporated the custom datatypes lowering pass into the primary TVM build passes. Once custom datatype lowering is incorporated into these passes, we will not need to do this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WNSbuvkUP8i"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    schedule = tvm.create_schedule([Z.op])\n",
    "    lowered_func = tvm.lower(schedule, [X, Y, Z])\n",
    "    lowered_func = tvm.ir_pass.LowerCustomDatatypes(lowered_func, target)\n",
    "    built_program = tvm.build(lowered_func, target=target)\n",
    "except tvm.TVMError as e:\n",
    "    # Print last line of error\n",
    "    print(str(e).split('\\n')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVsPSXRQUP8l"
   },
   "source": [
    "Now, trying to compile this program throws an error:\n",
    "`TVMError: Check failed: lower: Cast lowering function for target llvm destination type 129 source type 2 not found`.\n",
    "Let's dissect this error.\n",
    "\n",
    "The error is occurring during our `LowerCustomDatatypes(...)` call. TVM is telling us that it cannot find a _lowering function_ for the `Cast` operation, when casting from source type 2 (`float`, in TVM), to destination type 129 (our custom datatype). When lowering custom datatypes, if TVM encounters an operation over a custom datatype, it looks for a user-registered _lowering function_, which tells it how to lower the operation to an operation over datatypes it understands. We have not told TVM how to lower `Cast` operations for our custom datatypes; thus, the source of this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtbjLKUIUP8m"
   },
   "source": [
    "To fix this error, we simply need to specify a lowering function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETiJMdqTUP8n"
   },
   "outputs": [],
   "source": [
    "tvm.datatype.register_op(tvm.datatype.create_lower_func(\"FloatToBFloat16_wrapper\"),\n",
    "                         \"Cast\", target, \"bfloat\", \"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84bDvz9fUP8q"
   },
   "source": [
    "The `register_op(...)` call takes a lowering function, and a number of parameters which specify exactly the operation which should be lowered with the provided lowering function. In this case, the arguments we pass specify that this lowering function is for lowering a `Cast` from `float` to `bfloat` for target `\"llvm\"`.\n",
    "\n",
    "The lowering function passed into this call is very general: it should take an operation of the specified type (in this case, `Cast`) and return another operation which only uses datatypes which TVM understands.\n",
    "\n",
    "In the general case, we expect users to implement operations over their custom datatypes using calls to an external library. In our example, our `bfloat16` library (which, remember, is built into TVM) implements a `Cast` from `float` to `bfloat` in the function `FloatToBFloat16_wrapper`. To provide for the general case, we have made a helper function, `create_lower_func(...)`, which does just this: given a function name, it replaces the given operation with a `Call` to the function name provided. It additionally removes usages of the custom datatype by storing the custom datatype in an opaque `uint` of the appropriate width; in our case, a `uint16_t`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaFggLxIUP8q"
   },
   "source": [
    "We can now re-try our build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vUIx6jeUP8r"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    schedule = tvm.create_schedule([Z.op])\n",
    "    lowered_func = tvm.lower(schedule, [X, Y, Z])\n",
    "    lowered_func = tvm.ir_pass.LowerCustomDatatypes(lowered_func, target)\n",
    "    built_program = tvm.build(lowered_func, target=target)\n",
    "except tvm.TVMError as e:\n",
    "    # Print last line of error\n",
    "    print(str(e).split('\\n')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stqafBUoUP8u"
   },
   "source": [
    "This new error tells us that the `Add` lowering function is not found, which is good news, as it's no longer complaining about the `Cast`! We know what to do from here: we just need to register the lowering functions for the other operations in our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKXYgL1WUP8u"
   },
   "outputs": [],
   "source": [
    "tvm.datatype.register_op(tvm.datatype.create_lower_func(\"BFloat16ToFloat_wrapper\"),\n",
    "                         \"Cast\", target, \"float\", \"bfloat\")\n",
    "tvm.datatype.register_op(tvm.datatype.create_lower_func(\"BFloat16Add_wrapper\"),\n",
    "                         \"Add\", target, \"bfloat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQxwXrvLUP8x"
   },
   "source": [
    "Now, we can build our program without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RFjZ0vAUP8z"
   },
   "outputs": [],
   "source": [
    "schedule = tvm.create_schedule([Z.op])\n",
    "lowered_func = tvm.lower(schedule, [X, Y, Z])\n",
    "lowered_func = tvm.ir_pass.LowerCustomDatatypes(lowered_func, target)\n",
    "built_program = tvm.build(lowered_func, target=target)\n",
    "\n",
    "print(lowered_func.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-72gnM2UP82"
   },
   "source": [
    "You can see that the IR contains our program, and implements the program using calls to our library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-SExcyRUP82"
   },
   "source": [
    "Finally, we'll run the resulting program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGNIO9_2UP83"
   },
   "outputs": [],
   "source": [
    "z_bfloat = tvm.nd.empty(Z.shape, dtype=Z.dtype, ctx=context)\n",
    "built_program(x, y, z_bfloat)\n",
    "print(\"z_bfloat: {}\".format(z_bfloat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJCHb9ypUP86"
   },
   "source": [
    "**NOTE:** The external library functions implementing your datatype (e.g. `FloatToBFloat16_wrapper`, in our example) must be loaded into the process space and visible for lookup at runtime. In our example, this happens automatically, as the library is built into the TVM shared library object. However, in other cases, you can use `CDLL` to load your library in global mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzmRQ6otUP86"
   },
   "outputs": [],
   "source": [
    "# import ctypes\n",
    "# ctypes.CDLL(library_name, ctypes.RTLD_GLOBAL) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5X-XWm-KUP89"
   },
   "source": [
    "We can now look at the results of the two programs side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_FpH-N6UP89"
   },
   "outputs": [],
   "source": [
    "print(\"x:\\t\\t{}\".format(x))\n",
    "print(\"y:\\t\\t{}\".format(y))\n",
    "print(\"z:\\t\\t{}\".format(z))\n",
    "print(\"z_bfloat:\\t{}\".format(z_bfloat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqrIBhGwUP9A"
   },
   "source": [
    "Perhaps as expected, the `bfloat16` results are very close to the `float` results, but with some loss in precision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ACtzZp6UP9A"
   },
   "source": [
    "## Preview: Running Models With Custom Datatypes\n",
    "\n",
    "**Note:** All code previous to this point will work if you build TVM's `master` branch. This section of the notebook, however, uses code which is not yet merged into mainline TVM. This code still has its bugs (specifically, some numerical stability issues with posit softmax), and is moreso meant to demonstrate at a high level how you will be able to change a model to a custom datatype.\n",
    "\n",
    "In this final section of the notebook, we will demo additions to the Bring Your Own Datatypes framework which make it easy to run entire models using custom datatypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ecx9FzlOUP9B"
   },
   "source": [
    "### Interlude 2: Posits\n",
    "\n",
    "[Posits](https://posithub.org/docs/Posits4.pdf) are a new numerical datatype developed by John Gustafson; see [posithub.org](https://posithub.org/) for a central repository of all information about posits. Posits encode real numbers in a familiar way, using a sign, exponent, and fraction field. However, posits also add an additional _regime_ field, which provide an additional scaling factor on the number. There are a number of interesting practical results of this. A few highlights are:\n",
    "- Posits have greater precision near ±1, and less precision at very large values \n",
    "- Posits represent more numbers around ±1, and less numbers at very large values\n",
    "\n",
    "Both of these features make posits very attractive for deep learning! [Deep Positron](https://arxiv.org/abs/1812.01762) is an example of posit research in the deep learning space, demonstrating promising results when using posits instead of floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnV17w8tUP9B"
   },
   "source": [
    "### Converting Models to Custom Datatypes\n",
    "\n",
    "We will first choose the model which we would like to run with posits. In this case we use [Mobilenet](https://arxiv.org/abs/1704.04861). We choose Mobilenet due to its small size. In this alpha state of the Bring Your Own Datatypes framework, we have not implemented any software optimizations for running software emulations of custom datatypes; the result is poor performance due to many calls into our datatype emulation library.\n",
    "\n",
    "Relay has packaged up many models within its [python/tvm/relay/testing](https://github.com/dmlc/tvm/tree/master/python/tvm/relay/testing) directory. We will go ahead and grab Mobilenet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qocTy2P_UP9C"
   },
   "outputs": [],
   "source": [
    "from tvm.relay.testing.mobilenet import get_workload as get_mobilenet\n",
    "\n",
    "module, params = get_mobilenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4m2cZ_sUP9E"
   },
   "source": [
    "We can execute Mobilenet easily using the Relay graph execution engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbJ1QKrYUP9F"
   },
   "outputs": [],
   "source": [
    "ex = tvm.relay.create_executor(\"graph\", mod=module)\n",
    "input = tvm.nd.array(np.random.rand(3, 224, 224).astype(\"float32\"))\n",
    "result = ex.evaluate()(input, **params)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9WLoUbyUP9H"
   },
   "source": [
    "Now, we would like to change the model to use posits internally. To do so, we first must register posits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvkA1qmzUP9I"
   },
   "outputs": [],
   "source": [
    "tvm.datatype.register(\"posit\", 130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpTsbW7lUP9K"
   },
   "source": [
    "Next, we need to convert the network. To do this, we first define a function which will help us convert tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agghCRhZUP9L"
   },
   "outputs": [],
   "source": [
    "def convert_ndarray(dst_dtype, array, executor):\n",
    "    x = tvm.relay.var('x', shape=array.shape, dtype=str(array.dtype))\n",
    "    cast = tvm.relay.Function([x], x.astype(dst_dtype))\n",
    "    return executor.evaluate(cast)(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40DeWdNMUP9O"
   },
   "source": [
    "Now, to actually convert the entire network, we have written [a pass in Relay](https://github.com/gussmith23/tvm/blob/ea174c01c54a2529e19ca71e125f5884e728da6e/python/tvm/relay/frontend/change_datatype.py#L21) which simply converts all nodes within the model to use the new datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsTvKV10UP9P"
   },
   "outputs": [],
   "source": [
    "from tvm.relay.frontend.change_datatype import ChangeDatatype\n",
    "\n",
    "src_dtype = 'float32'                                                                                                                                                                \n",
    "dst_dtype = 'custom[posit]16'\n",
    "\n",
    "# Currently, custom datatypes only work if you run simplify_inference beforehand\n",
    "module = tvm.relay.transform.SimplifyInference()(module)\n",
    "\n",
    "# Run type inference before changing datatype\n",
    "module = tvm.relay.transform.InferType()(module)\n",
    "\n",
    "# Change datatype from float to posit and re-infer types\n",
    "cdtype = ChangeDatatype(src_dtype, dst_dtype)\n",
    "expr = cdtype.visit(module['main'])\n",
    "module = tvm.relay.transform.InferType()(tvm.relay.Module.from_expr(expr))\n",
    "\n",
    "# Finally, try to convert the parameters:\n",
    "try:\n",
    "  params = dict(\n",
    "      (p, convert_ndarray(dst_dtype, params[p], ex)) for p in params)\n",
    "except tvm.TVMError as e:\n",
    "    # Print last line of error\n",
    "    print(str(e).split('\\n')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ORDcLtJUP9R"
   },
   "source": [
    "When we attempt to convert the parameters, we get a familiar error. We need to implement our posit functions! \n",
    "\n",
    "Because this is a neural network, many more operations are required. We have implemented these operations in a small posit library at [3rdparty/bfloat16/bfloat16.cc](https://github.com/gussmith23/tvm/blob/a335e112dfa36fb7b460619401264ddb90007f55/3rdparty/bfloat16/bfloat16.cc). This small library depends on [Stillwater Supercomputing's Universal library](https://github.com/stillwater-sc/universal), which implements posits (and other _universal numbers_) in great detail.\n",
    "\n",
    "Here, we register all the needed functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4O4AKeHUP9T"
   },
   "outputs": [],
   "source": [
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"FloatToPosit16es1\"), \"Cast\",\n",
    "    \"llvm\", \"posit\", \"float\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1ToFloat\"), \"Cast\",\n",
    "    \"llvm\", \"float\", \"posit\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"IntToPosit16es1\"), \"Cast\",\n",
    "    \"llvm\", \"posit\", \"int\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1Add\"), \"Add\",\n",
    "    \"llvm\", \"posit\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1Sub\"), \"Sub\",\n",
    "    \"llvm\", \"posit\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"FloatToPosit16es1\"),\n",
    "    \"FloatImm\", \"llvm\", \"posit\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1Mul\"), \"Mul\",\n",
    "    \"llvm\", \"posit\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1Div\"), \"Div\",\n",
    "    \"llvm\", \"posit\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1Max\"), \"Max\",\n",
    "    \"llvm\", \"posit\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1Sqrt\"),\n",
    "    \"Call\",\n",
    "    \"llvm\",\n",
    "    \"posit\",\n",
    "    intrinsic_name=\"sqrt\")\n",
    "# TODO(gus) not sure if this will work...\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.lower_ite,\n",
    "    \"Call\",\n",
    "    \"llvm\",\n",
    "    \"posit\",\n",
    "    intrinsic_name=\"tvm_if_then_else\")\n",
    "tvm.datatype.register_op(\n",
    "    tvm.datatype.create_lower_func(\"Posit16es1Exp\"),\n",
    "    \"Call\",\n",
    "    \"llvm\",\n",
    "    \"posit\",\n",
    "    intrinsic_name=\"exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aflNnClJUP9V"
   },
   "source": [
    "Now, we can convert our params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVAeFXX3UP9X"
   },
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    (p, convert_ndarray(dst_dtype, params[p], ex)) for p in params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgQmep2ZUP9Y"
   },
   "source": [
    "We also need to convert our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BU5S8jwXUP9Z"
   },
   "outputs": [],
   "source": [
    "input = convert_ndarray(dst_dtype, input, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilSoA5S3UP9b"
   },
   "source": [
    "Finally, we can run the converted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj2qLnDcUP9c"
   },
   "outputs": [],
   "source": [
    "# Vectorization is not implemented with custom datatypes.\n",
    "with tvm.build_config(disable_vectorize=True):\n",
    "  result_posit = ex.evaluate(expr)(input, **params)\n",
    "  # Print the result! (first, convert back to float)\n",
    "  print(convert_ndarray(src_dtype, result_posit, ex))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "08_TVM_Tutorial_BringYourOwnDatatypes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
